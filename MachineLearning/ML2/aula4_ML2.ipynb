{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo aula 4 de Machine Learning 2 ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métodos de Agregação: Têm como procedimento geral construir **diversos estimadores independentes e usar a combinação desses estimadores para produzir a predição final**. No caso de problemas de regressão, essa combinação pode ser a média aritmética ou ponderada das predições individuais, enquanto que no caso de problemas de classificação, a decisão pode ser tomada por meio da votação majoritária (hard voting) ou então pela média arimtética ou ponderada das probabilidades de pertencimento de cada classe (soft voting) O principal objetivo do método é de reduzir variância, de modo que o modelo final seja melhor que todos os modelos individuais. Bons exemplos desses modelos são o Random Forest e o Extra Trees.\n",
    "\n",
    "Métodos de Boosting: constroem modelos robustos por meio da **combinação sequencial**, de modo que os **estimadores posteriores tentam reduzir o viés do estimador conjunto**, levando em **consideração os erros cometidos pelos estimadores anteriores**. Exemplos incluem o Gradient Boosting e o AdaBoost.\n",
    "\n",
    "Métodos de Stacking: consistem em **\"modelos empilhados\" de forma hierárquica**. Ou seja, **o modelo \"filho\" utiliza como entrada as saídas dos modelos \"pais\"**.[Este link](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/) tem mais detalhes sobre a aplicação do método de *stacking ensemble*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O *AdaBoost* significa **Adaptative Boosting**, e tem como procedimento geral a criação sucessiva dos chamados *weak learners*,ou modelos de aprendizagem fraca. Um exemplo de modelo de aprendizagem fraca no caso de modelos em árvores de um único nó (**stumps**).\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1744/1*nJ5VrsiS1yaOR77d4h8gyw.png\" width=700>\n",
    "\n",
    "<img src=\"https://static.packt-cdn.com/products/9781788295758/graphics/image_04_046-1.png\" width=500>\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Zhuo_Wang8/publication/288699540/figure/fig9/AS:668373486686246@1536364065786/Illustration-of-AdaBoost-algorithm-for-creating-a-strong-classifier-based-on-multiple.png\" width=500>\n",
    "\n",
    "sobre o AdaBoosting:\n",
    "\n",
    "De forma resumida, as principais ideias por trás deste algoritmo são:\n",
    "\n",
    "os algoritmos criam e combinam um conjunto de modelos fracos (em geral, stumps).\n",
    "\n",
    "cada stumps é criado levando em consideração os erros do stump anterior\n",
    "\n",
    "alguns dos stumps têm maior peso de decisão do que outros na predição do que outros na predição final\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "O bootstrapping é especialmente útil quando o tamanho da amostra é pequeno ou quando a distribuição das estatísticas não é conhecida ou não segue uma distribuição teórica. Essa técnica permite inferências mais precisas e pode ser aplicada em uma variedade de situações, incluindo regressão, análise de séries temporais, testes de hipóteses, entre outros."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo prático de função para avaliação do score:\n",
    "\n",
    "definindo a função de lucro que usaremos como métrica\n",
    "\n",
    "# def lucro_f(y_real, y_pred):\n",
    "\n",
    "    # cálculo da matriz de confusão\n",
    "    cm = confusion_matrix(y_real, y_pred)\n",
    "\n",
    "    # extração dos valores de falsos positivos e falsos negativos\n",
    "    FP = cm[0][1]                # falsos positivos\n",
    "    TP = cm[1][1]                # verdadeiros positivos\n",
    "    TN = cm[0][0]                # verdadeiros negativos\n",
    "    FN = cm[1][0]                # falsos negativos\n",
    "\n",
    "    # cálculo do lucro segundo a regra de negócios\n",
    "    # 100 euros por cliente verdadeiro positivo\n",
    "    # -1500 euros por cliente falso positivo\n",
    "    # 500 euros por cliente verdadeiro falso negativo\n",
    "    # -50 euros por cliente falso negativo\n",
    "    lucro = 100*TP - 150*FP + 500*TN - 50*FN\n",
    "\n",
    "    return lucro\n",
    "\n",
    "score_lucro = make_scorer(lucro_f, greater_is_better=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o Gradient Boosting, que implementa o princípio de boosting por meio de um **gradiente explícito**\n",
    "\n",
    "<img src=https://miro.medium.com/max/788/1*pEu2LNmxf9ttXHIALPcEBw.png width=600>\n",
    "\n",
    "A ideia é que caminhemos na direção do **erro mínimo** de maneira iterativa **passo a passo**.\n",
    "\n",
    "Este caminho se dá justamente pelo **gradiente** da **função de custo/perda**, que mede justamente os erros cometidos.\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif width=400>\n",
    "\n",
    "Este método é conhecido como:\n",
    "\n",
    "### Gradiente descendente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo geral do método é bem simples: determinar quais são os parâmetros da hipótese que minimizam a função de custo/perda. Para isso, o método \"percorre\" a função de erro, indo em direção ao seu mínimo (e este \"caminho\" feito na função se dá justamente pela determinação iterativa dos parâmetros, isto é, a cada passo, chegamos mais perto dos parâmetros finais da hipótese, conforme eles são ajustados aos dados.\n",
    "\n",
    "Pequeno interlúdio matemático: o gradiente descendente implementado pelo gradient boosting é, na verdade, um gradiente descendente funcional, isto é, desejamos encontrar não um conjunto de parâmetros que minimiza o erro, mas sim introduzir sequencialmente weak learners (hipótese simples) que minimizam o erro. Desta forma, o gradient boosting minimiza a função de custo ao ecolher iterativamente hipóteses simples que apontam na direção do mínimo, neste espaço funcional."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pra quem quiser saber um pouco mais de detalhes (e se aventurar na matemática), sugiro [este post](https://www.gormanalysis.com/blog/gradient-boosting-explained/) ou então [este site](https://explained.ai/gradient-boosting/), que contém vários materiais ótimos para entender o método com todos os detalhes matemáticos.\n",
    "\n",
    "Os [vídeos do StatQuest](https://www.youtube.com/playlist?list=PLblh5JKOoLUJjeXUvUE0maghNuY2_5fY6) também são uma boa referência"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para quem quiser se aventurar mais, sugiro algumas boas leituras:\n",
    "\n",
    "- [Este](https://shirinsplayground.netlify.app/2018/11/ml_basics_gbm/), explica bem as particularidades do XGBoost, além de dar uma boa introdução ao gradient boosting\n",
    "\n",
    "- [Este](https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb), introduz bem o método, enquanto enfativa suas particularidades, com alguns detalhes matemáticos;\n",
    "\n",
    "- [Este](https://xgboost.readthedocs.io/en/latest/tutorials/model.html), da própria documentação da biblioteca, traz uma explicação legal, e com alguns detalhes matemáticos;\n",
    "\n",
    "- [Este](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d), com uma discussão mais alto-nível (sem tantos detalhes) sobre o XGBoost e os "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E os principais hiperparâmetros a serem ajustados são:\n",
    "\n",
    "- `n_estimators` : novamente, o número de weak learners encadeados.(quantidade de árvores)\n",
    "\n",
    "- `learning_rate` : a constante que multiplica o gradiente no gradiente descendente. Essencialmente, controla o \"tamanho do passo\" a ser dado em direção ao mínimo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3 (DESAFIO)\n",
    "\n",
    "Agora é com vocês ! Existe uma base de dados que descrever a produtividade de uma indústria, portanto sendo um problema de regressão. Nessa atividade você deverá:\n",
    "\n",
    "- carregar a base de dados\n",
    "- identificar a necessidade de tratar dados nulos\n",
    "- calcule o lucro no pior cenário, melhor cenário e no cenário \"ingênuo\" - mais detalhes da função lucro abaixo\n",
    "- configurar um processador de dados usando o `ColumnTransformer`, `SimpleImputer` e `Pipeline`, conforme você determinar\n",
    "- separar uma base de treino e de teste, colocando 30 % para base de teste, sem estratificação\n",
    "- configure um amostrador de validação cruzada de 5 folds sem estratificação (`KFold`)\n",
    "- configure um `Pipeline` com o processamento, scaler (`StandardScaler`) e o modelo selecionado\n",
    "- use um seletor de modelos `RandomizedSearchCV`\n",
    "- teste os modelos `AdaBoostRegressor`, `GradientBoostingRegressor`, `XGBoostRegressor`\n",
    "- você tem a liberdade de escolher os hiperparâmetros e estimadores base\n",
    "- aplique a seguinte regra de negócios numa função para determinar o lucro\n",
    "\n",
    "    - se o erro for menor que -10 % em relação ao valor, deve-se pagar a execução do modelo (150) e o prejuízo de superestimar (p*2500)\n",
    "    - se o erro ficar entre -10 % e -1 % em relação ao valor, paga-se a execução do modelo (150) e metade do prejuízo (p*1250)\n",
    "    - se o erro ficar entre -1 % e + 1 % em relação ao valor, paga-se a execução do modelo (150)\n",
    "    - se o erro ficar entre 1 % e e 10 %, paga-se o modelo (150) porém ganha-se metade do lucro total (p*1000)\n",
    "    - se o erro for maior que 10 %, paga-se o modelo (150) porém se ganha o lucro total (p*2000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier,AdaBoostRegressor,GradientBoostingClassifier,GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, classification_report, ConfusionMatrixDisplay, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>quarter</th>\n",
       "      <th>department</th>\n",
       "      <th>day</th>\n",
       "      <th>team</th>\n",
       "      <th>targeted_productivity</th>\n",
       "      <th>smv</th>\n",
       "      <th>wip</th>\n",
       "      <th>over_time</th>\n",
       "      <th>incentive</th>\n",
       "      <th>idle_time</th>\n",
       "      <th>idle_men</th>\n",
       "      <th>no_of_style_change</th>\n",
       "      <th>no_of_workers</th>\n",
       "      <th>actual_productivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>Quarter1</td>\n",
       "      <td>sweing</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>8</td>\n",
       "      <td>0.80</td>\n",
       "      <td>26.16</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>7080</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.940725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>Quarter1</td>\n",
       "      <td>finishing</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>960</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.886500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>Quarter1</td>\n",
       "      <td>sweing</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>11</td>\n",
       "      <td>0.80</td>\n",
       "      <td>11.41</td>\n",
       "      <td>968.0</td>\n",
       "      <td>3660</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.800570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>Quarter1</td>\n",
       "      <td>sweing</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>12</td>\n",
       "      <td>0.80</td>\n",
       "      <td>11.41</td>\n",
       "      <td>968.0</td>\n",
       "      <td>3660</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.800570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>Quarter1</td>\n",
       "      <td>sweing</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>6</td>\n",
       "      <td>0.80</td>\n",
       "      <td>25.90</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>1920</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.800382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>3/11/2015</td>\n",
       "      <td>Quarter2</td>\n",
       "      <td>finishing</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>10</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>960</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.628333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>3/11/2015</td>\n",
       "      <td>Quarter2</td>\n",
       "      <td>finishing</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>8</td>\n",
       "      <td>0.70</td>\n",
       "      <td>3.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>960</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.625625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>3/11/2015</td>\n",
       "      <td>Quarter2</td>\n",
       "      <td>finishing</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>7</td>\n",
       "      <td>0.65</td>\n",
       "      <td>3.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>960</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.625625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>3/11/2015</td>\n",
       "      <td>Quarter2</td>\n",
       "      <td>finishing</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>9</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.505889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>3/11/2015</td>\n",
       "      <td>Quarter2</td>\n",
       "      <td>finishing</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>720</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.394722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1197 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date   quarter  department        day team  targeted_productivity  \\\n",
       "0      1/1/2015  Quarter1      sweing   Thursday    8                   0.80   \n",
       "1      1/1/2015  Quarter1  finishing    Thursday    1                   0.75   \n",
       "2      1/1/2015  Quarter1      sweing   Thursday   11                   0.80   \n",
       "3      1/1/2015  Quarter1      sweing   Thursday   12                   0.80   \n",
       "4      1/1/2015  Quarter1      sweing   Thursday    6                   0.80   \n",
       "...         ...       ...         ...        ...  ...                    ...   \n",
       "1192  3/11/2015  Quarter2   finishing  Wednesday   10                   0.75   \n",
       "1193  3/11/2015  Quarter2   finishing  Wednesday    8                   0.70   \n",
       "1194  3/11/2015  Quarter2   finishing  Wednesday    7                   0.65   \n",
       "1195  3/11/2015  Quarter2   finishing  Wednesday    9                   0.75   \n",
       "1196  3/11/2015  Quarter2   finishing  Wednesday    6                   0.70   \n",
       "\n",
       "        smv     wip  over_time  incentive  idle_time  idle_men  \\\n",
       "0     26.16  1108.0       7080         98        0.0         0   \n",
       "1      3.94     NaN        960          0        0.0         0   \n",
       "2     11.41   968.0       3660         50        0.0         0   \n",
       "3     11.41   968.0       3660         50        0.0         0   \n",
       "4     25.90  1170.0       1920         50        0.0         0   \n",
       "...     ...     ...        ...        ...        ...       ...   \n",
       "1192   2.90     NaN        960          0        0.0         0   \n",
       "1193   3.90     NaN        960          0        0.0         0   \n",
       "1194   3.90     NaN        960          0        0.0         0   \n",
       "1195   2.90     NaN       1800          0        0.0         0   \n",
       "1196   2.90     NaN        720          0        0.0         0   \n",
       "\n",
       "      no_of_style_change  no_of_workers  actual_productivity  \n",
       "0                      0           59.0             0.940725  \n",
       "1                      0            8.0             0.886500  \n",
       "2                      0           30.5             0.800570  \n",
       "3                      0           30.5             0.800570  \n",
       "4                      0           56.0             0.800382  \n",
       "...                  ...            ...                  ...  \n",
       "1192                   0            8.0             0.628333  \n",
       "1193                   0            8.0             0.625625  \n",
       "1194                   0            8.0             0.625625  \n",
       "1195                   0           15.0             0.505889  \n",
       "1196                   0            6.0             0.394722  \n",
       "\n",
       "[1197 rows x 15 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def penalty(x):\n",
    "    '''função que calcula a penalidade dependendo do erro'''\n",
    "    if x < -0.1:\n",
    "        return -2500\n",
    "    elif ((x >= -0.1) & (x < -0.01)):\n",
    "        return -1250\n",
    "    elif ((x >= -0.01) & (x < 0.01)):\n",
    "        return 0\n",
    "    elif ((x >= 0.01) & (x < 0.1)):\n",
    "        return 1000\n",
    "    else:\n",
    "        return 2000\n",
    "def lucro(y_real, ypred):\n",
    "    '''a função de lucro utilizada no make_scorer, recebe o dado (real,predito) do modelo que está sendo treinado, e retorna os valores\n",
    "    nas colunas criadas'''\n",
    "\n",
    "    data = pd.DataFrame(y_real.values, columns = ['real'])\n",
    "    data['pred'] = ypred\n",
    "\n",
    "    data['erro_p'] = (data['real'] - data['pred']) / data['real']\n",
    "\n",
    "    data['penalty'] = data.erro_p.apply(penalty)\n",
    "    data['penalty'] -= 150\n",
    "\n",
    "    print(data.head())\n",
    "\n",
    "    return data.penalty.sum()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('garments_worker_productivity.csv')\n",
    "data['team']= data['team'].astype('str')\n",
    "\n",
    "x = data.drop(columns=['date','actual_productivity'], axis = 1)\n",
    "y = data['actual_productivity']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, shuffle=True, random_state= 42)\n",
    "\n",
    "pipe_cat = Pipeline([('ohe', OneHotEncoder(drop='first'))])\n",
    "pipe_num = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value=0))])\n",
    "\n",
    "\n",
    "data_cat = x_train.select_dtypes(exclude=np.number).columns.tolist()\n",
    "data_num = x_train.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "ct = ColumnTransformer([('cat_transform', pipe_cat, data_cat),\n",
    "                        ('num_transformer', pipe_num, data_num)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_final = Pipeline([('pre_processing', ct),\n",
    "                       ('std_scaler', StandardScaler()),\n",
    "                       ('ada_boost', AdaBoostRegressor())])\n",
    "\n",
    "kf3 = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "param_dict = {'ada_boost__n_estimators': np.random.randint(2, 1000, size = 500)}\n",
    "\n",
    "\n",
    "model = RandomizedSearchCV(estimator=pipe_final, param_distributions=param_dict, n_iter=5, scoring=make_scorer(lucro, greater_is_better=True), cv=kf3,refit=True , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       real      pred    erro_p  penalty\n",
      "0  0.314167  0.711061 -1.263325    -2650\n",
      "1  0.800246  0.783450  0.020988      850\n",
      "2  0.966759  0.832331  0.139051     1850\n",
      "3  0.535678  0.452684  0.154932     1850\n",
      "4  0.902500  0.653662  0.275721     1850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.640578  0.697820 -0.089360    -1400\n",
      "1  0.799983  0.664953  0.168791     1850\n",
      "2  0.578314  0.754796 -0.305165    -2650\n",
      "3  0.598627  0.670579 -0.120195    -2650\n",
      "4  0.656667  0.664628 -0.012124    -1400\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.800000  0.798029  0.002464     -150\n",
      "1  0.999995  0.851542  0.148454     1850\n",
      "2  0.556562  0.624112 -0.121369    -2650\n",
      "3  0.750396  0.767804 -0.023199    -1400\n",
      "4  0.850224  0.798029  0.061390      850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.314167  0.723061 -1.301519    -2650\n",
      "1  0.800246  0.770813  0.036780      850\n",
      "2  0.966759  0.841233  0.129843     1850\n",
      "3  0.535678  0.464335  0.133183     1850\n",
      "4  0.902500  0.636895  0.294299     1850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.640578  0.729728 -0.139171    -2650\n",
      "1  0.799983  0.775435  0.030685      850\n",
      "2  0.578314  0.764337 -0.321664    -2650\n",
      "3  0.598627  0.775435 -0.295356    -2650\n",
      "4  0.656667  0.670883 -0.021648    -1400\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.800000  0.787014  0.016232      850\n",
      "1  0.999995  0.822210  0.177786     1850\n",
      "2  0.556562  0.623273 -0.119861    -2650\n",
      "3  0.750396  0.781374 -0.041283    -1400\n",
      "4  0.850224  0.787014  0.074344      850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.314167  0.693112 -1.206193    -2650\n",
      "1  0.800246  0.790789  0.011818      850\n",
      "2  0.966759  0.824687  0.146957     1850\n",
      "3  0.535678  0.480887  0.102283     1850\n",
      "4  0.902500  0.644976  0.285345     1850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.640578  0.679170 -0.060247    -1400\n",
      "1  0.799983  0.643449  0.195672     1850\n",
      "2  0.578314  0.712852 -0.232638    -2650\n",
      "3  0.598627  0.684615 -0.143641    -2650\n",
      "4  0.656667  0.616569  0.061062      850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.800000  0.804701 -0.005877     -150\n",
      "1  0.999995  0.848775  0.151221     1850\n",
      "2  0.556562  0.628695 -0.129604    -2650\n",
      "3  0.750396  0.733037  0.023132      850\n",
      "4  0.850224  0.804701  0.053542      850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.314167  0.718718 -1.287695    -2650\n",
      "1  0.800246  0.775236  0.031253      850\n",
      "2  0.966759  0.840323  0.130784     1850\n",
      "3  0.535678  0.488919  0.087290      850\n",
      "4  0.902500  0.693599  0.231469     1850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.640578  0.681500 -0.063883    -1400\n",
      "1  0.799983  0.676251  0.154668     1850\n",
      "2  0.578314  0.750817 -0.298286    -2650\n",
      "3  0.598627  0.621418 -0.038071    -1400\n",
      "4  0.656667  0.622283  0.052361      850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.800000  0.796214  0.004733     -150\n",
      "1  0.999995  0.821785  0.178211     1850\n",
      "2  0.556562  0.626761 -0.126129    -2650\n",
      "3  0.750396  0.792499 -0.056108    -1400\n",
      "4  0.850224  0.796214  0.063524      850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.314167  0.713902 -1.272366    -2650\n",
      "1  0.800246  0.776967  0.029090      850\n",
      "2  0.966759  0.840742  0.130350     1850\n",
      "3  0.535678  0.463826  0.134132     1850\n",
      "4  0.902500  0.655762  0.273394     1850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.640578  0.687605 -0.073414    -1400\n",
      "1  0.799983  0.707165  0.116024     1850\n",
      "2  0.578314  0.762884 -0.319151    -2650\n",
      "3  0.598627  0.687605 -0.148636    -2650\n",
      "4  0.656667  0.640195  0.025084      850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.800000  0.801935 -0.002419     -150\n",
      "1  0.999995  0.835043  0.164953     1850\n",
      "2  0.556562  0.606035 -0.088889    -1400\n",
      "3  0.750396  0.782625 -0.042950    -1400\n",
      "4  0.850224  0.801935  0.056795      850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=3, random_state=42, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[('pre_processing',\n",
       "                                              ColumnTransformer(transformers=[('cat_transform',\n",
       "                                                                               Pipeline(steps=[('ohe',\n",
       "                                                                                                OneHotEncoder(drop='first'))]),\n",
       "                                                                               ['quarter',\n",
       "                                                                                'department',\n",
       "                                                                                'day',\n",
       "                                                                                'team']),\n",
       "                                                                              ('num_transformer',\n",
       "                                                                               Pipeline(steps=[('imputer',\n",
       "                                                                                                SimpleImputer(fill_value=0,\n",
       "                                                                                                              strategy='constant'))]...\n",
       "       274, 354, 137, 223, 777, 274, 862, 534, 416,  86, 873, 302, 539,\n",
       "       488, 639, 857, 686, 901, 296, 494, 645,  46, 215, 452, 645, 477,\n",
       "       972, 685, 188, 380, 448, 603,  13, 326, 445, 301, 512, 602, 766,\n",
       "       600, 971,  66, 631, 108, 954,  61, 661, 335, 536, 199,  99, 891,\n",
       "       331, 841,  91, 290, 772, 512, 273, 994, 701, 334, 652, 288, 889,\n",
       "       541, 111, 652, 595, 948, 181, 307, 259, 902, 387, 493, 644, 784,\n",
       "       410, 844, 188, 413, 464, 898])},\n",
       "                   random_state=42, scoring=make_scorer(lucro))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = model.best_estimator_.predict(x_test)\n",
    "resultados = pd.DataFrame(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89483.33333333333"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       real      pred    erro_p  penalty\n",
      "0  0.268214  0.547032 -1.039532    -2650\n",
      "1  0.800359  0.789322  0.013789      850\n",
      "2  0.681061  0.656134  0.036599      850\n",
      "3  0.325000  0.619499 -0.906152    -2650\n",
      "4  0.667604  0.615967  0.077347      850\n",
      "175750\n"
     ]
    }
   ],
   "source": [
    "print(lucro(y_test,resultados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_gradient = Pipeline([('ct',ct),\n",
    "                          ('std_scaler', StandardScaler()),\n",
    "                          ('gboost', GradientBoostingRegressor())])\n",
    "\n",
    "param_gradient = {'gboost__n_estimators':np.random.randint(20,200,200),\n",
    "                  'gboost__learning_rate': np.random.uniform(0.01, 0.2, 100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gradient = RandomizedSearchCV(estimator=pipe_gradient, param_distributions=param_gradient, n_iter=5, refit=True, scoring=make_scorer(lucro, greater_is_better=True), cv=kf3,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       real      pred    erro_p  penalty\n",
      "0  0.314167  0.810939 -1.581238    -2650\n",
      "1  0.800246  0.806795 -0.008184     -150\n",
      "2  0.966759  0.821748  0.149997     1850\n",
      "3  0.535678  0.570597 -0.065186    -1400\n",
      "4  0.902500  0.791425  0.123075     1850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.640578  0.771713 -0.204714    -2650\n",
      "1  0.799983  0.763852  0.045164      850\n",
      "2  0.578314  0.790137 -0.366276    -2650\n",
      "3  0.598627  0.711700 -0.188886    -2650\n",
      "4  0.656667  0.761836 -0.160156    -2650\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.800000  0.833111 -0.041388    -1400\n",
      "1  0.999995  0.930109  0.069886      850\n",
      "2  0.556562  0.594501 -0.068165    -1400\n",
      "3  0.750396  0.753864 -0.004622     -150\n",
      "4  0.850224  0.833111  0.020128      850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.314167  0.857496 -1.729430    -2650\n",
      "1  0.800246  0.804372 -0.005156     -150\n",
      "2  0.966759  0.869472  0.100633     1850\n",
      "3  0.535678  0.528326  0.013724      850\n",
      "4  0.902500  0.781963  0.133559     1850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.640578  0.751171 -0.172647    -2650\n",
      "1  0.799983  0.754832  0.056439      850\n",
      "2  0.578314  0.822789 -0.422736    -2650\n",
      "3  0.598627  0.681004 -0.137609    -2650\n",
      "4  0.656667  0.754781 -0.149413    -2650\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.800000  0.841470 -0.051838    -1400\n",
      "1  0.999995  0.973356  0.026640      850\n",
      "2  0.556562  0.504124  0.094218      850\n",
      "3  0.750396  0.749807  0.000784     -150\n",
      "4  0.850224  0.859496 -0.010905    -1400\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.314167  0.904333 -1.878513    -2650\n",
      "1  0.800246  0.809134 -0.011107    -1400\n",
      "2  0.966759  0.877067  0.092776      850\n",
      "3  0.535678  0.546670 -0.020520    -1400\n",
      "4  0.902500  0.764615  0.152781     1850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.640578  0.708198 -0.105562    -2650\n",
      "1  0.799983  0.667991  0.164994     1850\n",
      "2  0.578314  0.847912 -0.466179    -2650\n",
      "3  0.598627  0.696813 -0.164018    -2650\n",
      "4  0.656667  0.702044 -0.069102    -1400\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.800000  0.847333 -0.059166    -1400\n",
      "1  0.999995  0.989770  0.010225      850\n",
      "2  0.556562  0.543907  0.022739      850\n",
      "3  0.750396  0.739772  0.014157      850\n",
      "4  0.850224  0.857631 -0.008713     -150\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.314167  0.877680 -1.793676    -2650\n",
      "1  0.800246  0.788846  0.014245      850\n",
      "2  0.966759  0.852932  0.117742     1850\n",
      "3  0.535678  0.547918 -0.022849    -1400\n",
      "4  0.902500  0.761367  0.156380     1850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.640578  0.694663 -0.084432    -1400\n",
      "1  0.799983  0.681036  0.148687     1850\n",
      "2  0.578314  0.868999 -0.502640    -2650\n",
      "3  0.598627  0.692531 -0.156864    -2650\n",
      "4  0.656667  0.713343 -0.086309    -1400\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.800000  0.838913 -0.048641    -1400\n",
      "1  0.999995  0.982253  0.017742      850\n",
      "2  0.556562  0.526020  0.054877      850\n",
      "3  0.750396  0.735175  0.020283      850\n",
      "4  0.850224  0.861341 -0.013075    -1400\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.314167  0.873304 -1.779748    -2650\n",
      "1  0.800246  0.803990 -0.004679     -150\n",
      "2  0.966759  0.877154  0.092686      850\n",
      "3  0.535678  0.519447  0.030300      850\n",
      "4  0.902500  0.804690  0.108376     1850\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.640578  0.713324 -0.113564    -2650\n",
      "1  0.799983  0.690103  0.137353     1850\n",
      "2  0.578314  0.839658 -0.451906    -2650\n",
      "3  0.598627  0.687007 -0.147637    -2650\n",
      "4  0.656667  0.733478 -0.116971    -2650\n",
      "       real      pred    erro_p  penalty\n",
      "0  0.800000  0.867428 -0.084285    -1400\n",
      "1  0.999995  0.982341  0.017654      850\n",
      "2  0.556562  0.530593  0.046661      850\n",
      "3  0.750396  0.748025  0.003159     -150\n",
      "4  0.850224  0.856859 -0.007804     -150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=3, random_state=42, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[('ct',\n",
       "                                              ColumnTransformer(transformers=[('cat_transform',\n",
       "                                                                               Pipeline(steps=[('ohe',\n",
       "                                                                                                OneHotEncoder(drop='first'))]),\n",
       "                                                                               ['quarter',\n",
       "                                                                                'department',\n",
       "                                                                                'day',\n",
       "                                                                                'team']),\n",
       "                                                                              ('num_transformer',\n",
       "                                                                               Pipeline(steps=[('imputer',\n",
       "                                                                                                SimpleImputer(fill_value=0,\n",
       "                                                                                                              strategy='constant'))]),\n",
       "                                                                               ['targeted...\n",
       "        85, 159, 111,  61, 192,  84,  88, 117,  98, 183, 122,  66,  80,\n",
       "       169, 110, 131,  51, 196, 191, 168,  99, 175,  67, 197, 165,  79,\n",
       "        98,  52, 187,  56, 122,  54,  43,  78,  97, 190, 129,  48,  50,\n",
       "       191, 123,  88, 130,  79, 142, 162, 147,  66, 147, 164, 137, 138,\n",
       "       122, 148,  65,  37,  53, 131,  46, 193,  21, 190,  80, 147, 170,\n",
       "        93,  27,  97, 111,  95,  97,  72,  87,  42,  93,  95, 105,  22,\n",
       "        57, 199,  64, 173, 178, 168, 174,  35, 143,  94, 130, 127,  43,\n",
       "        49, 102, 186,  33,  54])},\n",
       "                   scoring=make_scorer(lucro))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gradient.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52490601, 0.79720508, 0.68154621, 0.62437889, 0.66315248,\n",
       "       0.70879792, 0.55841364, 0.55841364, 0.64582048, 0.7574486 ,\n",
       "       0.71553168, 0.82324399, 0.81606095, 0.67720007, 0.85014234,\n",
       "       0.78468617, 0.92392035, 0.59417211, 0.59658472, 0.90075265,\n",
       "       0.92392035, 0.66744363, 0.82681562, 0.81621487, 0.82702222,\n",
       "       0.51583372, 0.67583246, 0.92601368, 0.59576492, 0.83181057,\n",
       "       0.91477386, 0.79534536, 0.82681562, 0.82836682, 0.71438273,\n",
       "       0.59576492, 0.71452083, 0.81606095, 0.80594611, 0.77164102,\n",
       "       0.73074782, 0.85014234, 0.67552553, 0.90075265, 0.7574486 ,\n",
       "       0.80594611, 0.62437889, 0.7880414 , 0.79525964, 0.7972486 ,\n",
       "       0.76821887, 0.80620788, 0.65593863, 0.81606095, 0.64533788,\n",
       "       0.65484646, 0.70226656, 0.73811197, 0.82120982, 0.69393225,\n",
       "       0.68154621, 0.80643513, 0.57833404, 0.81014722, 0.50608685,\n",
       "       0.66933065, 0.79720508, 0.81068155, 0.74266073, 0.59576492,\n",
       "       0.7326748 , 0.6295984 , 0.66744363, 0.75040186, 0.7574486 ,\n",
       "       0.57776555, 0.74266073, 0.68864746, 0.8347963 , 0.7913825 ,\n",
       "       0.79243519, 0.82324399, 0.83915894, 0.68154621, 0.71585597,\n",
       "       0.70494622, 0.66915307, 0.7972486 , 0.92601368, 0.59576492,\n",
       "       0.57776555, 0.78318387, 0.79867831, 0.68864746, 0.77492302,\n",
       "       0.79137377, 0.68864746, 0.8054602 , 0.69842508, 0.78227541,\n",
       "       0.79529631, 0.80620788, 0.92601368, 0.66315248, 0.7141883 ,\n",
       "       0.76389247, 0.68864746, 0.70937739, 0.7574486 , 0.79162294,\n",
       "       0.80620788, 0.8289839 , 0.79583659, 0.82681562, 0.66325136,\n",
       "       0.74266073, 0.85014234, 0.81606095, 0.68864746, 0.51501839,\n",
       "       0.4068586 , 0.83181057, 0.65413783, 0.57833404, 0.7574486 ,\n",
       "       0.73074782, 0.87809044, 0.88018378, 0.77146981, 0.79867831,\n",
       "       0.68154621, 0.65484646, 0.64582048, 0.76781117, 0.59576492,\n",
       "       0.80688057, 0.51501839, 0.74009778, 0.57404149, 0.7846571 ,\n",
       "       0.81411551, 0.7967961 , 0.63466085, 0.70494622, 0.66744363,\n",
       "       0.73074782, 0.50912465, 0.84201537, 0.59576492, 0.54045273,\n",
       "       0.80844079, 0.79583659, 0.81606095, 0.554463  , 0.64448103,\n",
       "       0.71585597, 0.71585597, 0.82324399, 0.91477386, 0.81845564,\n",
       "       0.52476526, 0.58519051, 0.7574486 , 0.59576492, 0.52476526,\n",
       "       0.70494622, 0.77146981, 0.68864746, 0.66234232, 0.77146981,\n",
       "       0.75820214, 0.73552476, 0.84866911, 0.59247294, 0.85014234,\n",
       "       0.79720508, 0.7972486 , 0.58669709, 0.85014234, 0.82681562,\n",
       "       0.80479572, 0.75036324, 0.68154621, 0.70226656, 0.70936455,\n",
       "       0.67233901, 0.54898292, 0.83958465, 0.81213275, 0.65413783,\n",
       "       0.68670203, 0.61572504, 0.68864746, 0.84419555, 0.84398283,\n",
       "       0.85014234, 0.56863149, 0.91541294, 0.70928142, 0.52476526,\n",
       "       0.77164102, 0.79966672, 0.4068586 , 0.81606095, 0.59433826,\n",
       "       0.7972486 , 0.79567041, 0.59576492, 0.79720508, 0.73074782,\n",
       "       0.66744363, 0.8077692 , 0.8077692 , 0.73643374, 0.71585597,\n",
       "       0.81606095, 0.77164102, 0.78947452, 0.79966672, 0.79243519,\n",
       "       0.73811197, 0.78906598, 0.71585597, 0.65593863, 0.7846571 ,\n",
       "       0.81621487, 0.84877118, 0.82324399, 0.68154621, 0.92601368,\n",
       "       0.50608685, 0.7972486 , 0.72120911, 0.81845564, 0.6055118 ,\n",
       "       0.70928142, 0.7760772 , 0.6142739 , 0.72229491, 0.78469556,\n",
       "       0.66744363, 0.8347963 , 0.66045747, 0.71452083, 0.80400068,\n",
       "       0.58669709, 0.77164102, 0.73811197, 0.82324399, 0.79246025,\n",
       "       0.69908959, 0.66191192, 0.73074782, 0.79525964, 0.62067231,\n",
       "       0.81606095, 0.81606095, 0.79867831, 0.81606095, 0.59433826,\n",
       "       0.6555408 , 0.75341698, 0.83958465, 0.51651864, 0.64877463,\n",
       "       0.57833404, 0.84002751, 0.66744363, 0.73668397, 0.59576492,\n",
       "       0.81411551, 0.69073274, 0.69082316, 0.75465122, 0.74266073,\n",
       "       0.7880414 , 0.79867831, 0.82574636, 0.80620788, 0.81606095,\n",
       "       0.79534536, 0.66234232, 0.79720508, 0.91884392, 0.77164102,\n",
       "       0.77934168, 0.81068155, 0.67452277, 0.80688057, 0.73811197,\n",
       "       0.80844079, 0.68941443, 0.4916918 , 0.75234729, 0.51967805,\n",
       "       0.66744363, 0.83954159, 0.50062334, 0.79720508, 0.71445104,\n",
       "       0.75456104, 0.79720508, 0.79966672, 0.84877118, 0.62603551,\n",
       "       0.83110743, 0.68154621, 0.77885487, 0.52643297, 0.66611527,\n",
       "       0.79720508, 0.70226656, 0.63466085, 0.73100173, 0.83620874,\n",
       "       0.79966672, 0.66744363, 0.77164102, 0.51583372, 0.62423497,\n",
       "       0.75456104, 0.64582048, 0.77280185, 0.80594611, 0.59576492,\n",
       "       0.82702222, 0.64582048, 0.80594611, 0.4863016 , 0.77736129,\n",
       "       0.79438062, 0.77615986, 0.66611527, 0.66744363, 0.72657464,\n",
       "       0.76386694, 0.7972486 , 0.80859502, 0.81606095, 0.75456104,\n",
       "       0.69908959, 0.73074782, 0.83612113, 0.72316407, 0.68864746,\n",
       "       0.92392035, 0.88018378, 0.47586622, 0.72563768, 0.7972486 ,\n",
       "       0.69073274, 0.79867831, 0.53762624, 0.78318387, 0.77146981,\n",
       "       0.7456429 , 0.85014234, 0.71928961, 0.7574486 , 0.80620788])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gradient.best_score_\n",
    "model_gradient.best_estimator_.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-39016.666666666664"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gradient.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
